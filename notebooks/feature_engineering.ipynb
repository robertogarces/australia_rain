{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ede32479",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ“Œ Feature Engineering Plan\n",
    "\n",
    "Based on the exploratory data analysis (EDA) conducted, we can now outline a structured plan to guide the feature engineering process. The following steps will be followed:\n",
    "\n",
    "1. **Create a `'Month'` Feature**  \n",
    "   Extract the month from the date column to capture potential seasonal trends in rainfall.\n",
    "\n",
    "2. **Remove Outliers**  \n",
    "   Identify and eliminate extreme values that may distort the modelâ€™s performance.\n",
    "\n",
    "3. **Address Skewness**  \n",
    "   Apply transformations (e.g., log, Box-Cox) to reduce skewness in numerical features where necessary.\n",
    "\n",
    "4. **Remove Highly Correlated Features**  \n",
    "   Drop redundant features that show strong correlation with each other to reduce multicollinearity.\n",
    "\n",
    "5. **Handle Missing Values**  \n",
    "   - If a feature has a high percentage of missing values, it will either be dropped or a new category such as `'Missing'` will be created (for categorical features).  \n",
    "   - If the percentage of missing values is moderate or low, different imputation strategies will be tested.\n",
    "\n",
    "6. **Create New Features**  \n",
    "   Engineer additional features based on domain knowledge or variable interactions that may improve model performance.\n",
    "\n",
    "7. **Standardization and Encoding**  \n",
    "   - Standardize numerical features to ensure consistent scale.  \n",
    "   - Apply suitable encoding techniques (e.g., One-Hot Encoding, Ordinal Encoding) to categorical features for compatibility with machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dad4fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4a1bf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (142193, 23)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>...</th>\n",
       "      <th>Humidity9am</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>Pressure3pm</th>\n",
       "      <th>Cloud9am</th>\n",
       "      <th>Cloud3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RainTomorrow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-12-01</td>\n",
       "      <td>Albury</td>\n",
       "      <td>13.4</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>44.0</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>71.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1007.7</td>\n",
       "      <td>1007.1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.9</td>\n",
       "      <td>21.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-12-02</td>\n",
       "      <td>Albury</td>\n",
       "      <td>7.4</td>\n",
       "      <td>25.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WNW</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1010.6</td>\n",
       "      <td>1007.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.2</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-12-03</td>\n",
       "      <td>Albury</td>\n",
       "      <td>12.9</td>\n",
       "      <td>25.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WSW</td>\n",
       "      <td>46.0</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1007.6</td>\n",
       "      <td>1008.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date Location  MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine  \\\n",
       "0  2008-12-01   Albury     13.4     22.9       0.6          NaN       NaN   \n",
       "1  2008-12-02   Albury      7.4     25.1       0.0          NaN       NaN   \n",
       "2  2008-12-03   Albury     12.9     25.7       0.0          NaN       NaN   \n",
       "\n",
       "  WindGustDir  WindGustSpeed WindDir9am  ... Humidity9am  Humidity3pm  \\\n",
       "0           W           44.0          W  ...        71.0         22.0   \n",
       "1         WNW           44.0        NNW  ...        44.0         25.0   \n",
       "2         WSW           46.0          W  ...        38.0         30.0   \n",
       "\n",
       "   Pressure9am  Pressure3pm  Cloud9am  Cloud3pm  Temp9am  Temp3pm  RainToday  \\\n",
       "0       1007.7       1007.1       8.0       NaN     16.9     21.8        0.0   \n",
       "1       1010.6       1007.8       NaN       NaN     17.2     24.3        0.0   \n",
       "2       1007.6       1008.7       NaN       2.0     21.0     23.2        0.0   \n",
       "\n",
       "   RainTomorrow  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "\n",
       "[3 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('../data/raw/weatherAUS.csv')\n",
    "target = 'RainTomorrow'\n",
    "# Drop target labels that are null\n",
    "df.dropna(subset=[target], inplace=True)\n",
    "# Convert both target and RainToday feature into boolean\n",
    "df['RainTomorrow'] = df['RainTomorrow'].map({'No': 0, 'Yes': 1})\n",
    "df['RainToday'] = df['RainToday'].map({'No': 0, 'Yes': 1})\n",
    "\n",
    "print(f'Dataframe shape: {df.shape}')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1b13c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "num_features = df.select_dtypes(include=np.number).columns.tolist()\n",
    "cat_features = df.select_dtypes(include='object').columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09eecb9c",
   "metadata": {},
   "source": [
    "#### 1. Create `'Month'` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f37dea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Month'] = df['Date'].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ee3fb8",
   "metadata": {},
   "source": [
    "#### 2. Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a458ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_with_outliers = ['Rainfall', 'Evaporation', 'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "114aa9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows removed due to upper 1.00% outliers: 4944\n"
     ]
    }
   ],
   "source": [
    "def filter_outliers_by_percentile(df, features, percentile=0.99):\n",
    "    \"\"\"\n",
    "    Remove rows where any value in specified features exceeds the given percentile threshold.\n",
    "    Only filters the upper tail (high values) as outliers.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        features (list): List of columns to check for outliers.\n",
    "        percentile (float): Percentile threshold (between 0 and 1). Rows with values above this percentile are removed.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame without rows exceeding the specified percentile in any of the features.\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    outlier_indices = set()\n",
    "    \n",
    "    for col in features:\n",
    "        threshold = df_clean[col].quantile(percentile)\n",
    "        col_outliers = df_clean[df_clean[col] > threshold].index\n",
    "        outlier_indices.update(col_outliers)\n",
    "        \n",
    "    initial_rows = df_clean.shape[0]\n",
    "    df_clean = df_clean.drop(index=outlier_indices)\n",
    "    final_rows = df_clean.shape[0]\n",
    "    \n",
    "    print(f\"Rows removed due to upper {100*(1-percentile):.2f}% outliers: {initial_rows - final_rows}\")\n",
    "    return df_clean\n",
    "\n",
    "# Ejemplo de uso:\n",
    "df = filter_outliers_by_percentile(df, features_with_outliers, 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f72e652",
   "metadata": {},
   "source": [
    "#### 3. Address skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d806824d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sunshine        -0.521518\n",
       "Humidity9am     -0.478696\n",
       "Cloud9am        -0.206948\n",
       "Cloud3pm        -0.203580\n",
       "Pressure9am     -0.054308\n",
       "Pressure3pm     -0.010067\n",
       "MinTemp          0.033117\n",
       "Humidity3pm      0.035499\n",
       "Temp9am          0.094830\n",
       "MaxTemp          0.221805\n",
       "Temp3pm          0.236211\n",
       "WindSpeed3pm     0.347202\n",
       "WindSpeed9am     0.441208\n",
       "WindGustSpeed    0.545855\n",
       "Evaporation      0.888658\n",
       "RainTomorrow     1.385824\n",
       "RainToday        1.399826\n",
       "Rainfall         4.047390\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[num_features].skew().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57ebb87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewed features: ['Rainfall', 'Evaporation', 'Sunshine', 'WindGustSpeed', 'RainToday']\n"
     ]
    }
   ],
   "source": [
    "def get_skewed_features(df, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Returns a Series of numerical features with skewness < -threshold or > threshold.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        threshold (float): Skewness threshold to detect features to be treated.\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: Features with strong skewness.\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    skew_values = df[numeric_cols].skew()\n",
    "    skewed = skew_values[(skew_values > threshold) | (skew_values < -threshold)]\n",
    "    return skewed.index.to_list()\n",
    "\n",
    "skewed_features = get_skewed_features(df, threshold=0.5)\n",
    "skewed_features.remove('RainTomorrow')\n",
    "print(\"Skewed features:\", skewed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "991e8fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "def treat_skewness(df, features):\n",
    "    \"\"\"\n",
    "    Apply transformations to reduce skewness in numeric features.\n",
    "    Uses Box-Cox for strictly positive data and Yeo-Johnson otherwise.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with numeric features.\n",
    "        features (list): List of feature names to transform.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with transformed features.\n",
    "    \"\"\"\n",
    "    df_transformed = df.copy()\n",
    "    \n",
    "    for feature in features:\n",
    "        data = df_transformed[feature].values.reshape(-1, 1)\n",
    "        if (df_transformed[feature] > 0).all():\n",
    "            # Use Box-Cox transformation\n",
    "            pt = PowerTransformer(method='box-cox')\n",
    "        else:\n",
    "            # Use Yeo-Johnson transformation for zero or negative values\n",
    "            pt = PowerTransformer(method='yeo-johnson')\n",
    "        \n",
    "        try:\n",
    "            df_transformed[feature] = pt.fit_transform(data).flatten()\n",
    "        except Exception as e:\n",
    "            print(f\"Could not transform feature {feature}: {e}\")\n",
    "    \n",
    "    return df_transformed\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# features_to_treat = ['Rainfall', 'Temperature', 'WindGustSpeed']\n",
    "df = treat_skewness(df, skewed_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e61ff78",
   "metadata": {},
   "source": [
    "#### 4. Remove highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71d9fad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped features: ['RainToday', 'Temp9am', 'MaxTemp', 'Pressure3pm']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def remove_highly_correlated_features(df, target, threshold=0.9, strategy='nulls'):\n",
    "    \"\"\"\n",
    "    Removes one of each pair of highly correlated features based on a selection strategy.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataset.\n",
    "        target (str): Name of the target variable.\n",
    "        threshold (float): Correlation threshold.\n",
    "        strategy (str): Strategy to drop one variable from each correlated pair. \n",
    "                        Options: 'nulls', 'target_corr'.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with reduced features.\n",
    "        list: List of dropped features.\n",
    "    \"\"\"\n",
    "    df_numeric = df.select_dtypes(include=[np.number]).drop(columns=[target], errors='ignore')\n",
    "    corr_matrix = df_numeric.corr().abs()\n",
    "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    to_drop = set()\n",
    "\n",
    "    for col in upper_tri.columns:\n",
    "        for row in upper_tri.index:\n",
    "            corr_value = upper_tri.loc[row, col]\n",
    "            if pd.notnull(corr_value) and corr_value > threshold:\n",
    "                # If any of the two has already been dropped, skip\n",
    "                if row in to_drop or col in to_drop:\n",
    "                    continue\n",
    "\n",
    "                if strategy == 'nulls':\n",
    "                    nulls_row = df[row].isnull().sum()\n",
    "                    nulls_col = df[col].isnull().sum()\n",
    "                    drop = row if nulls_row > nulls_col else col\n",
    "\n",
    "                elif strategy == 'target_corr':\n",
    "                    if target not in df.columns:\n",
    "                        raise ValueError(\"Target column not found in DataFrame.\")\n",
    "                    corr_row = abs(df[[row, target]].corr().iloc[0, 1])\n",
    "                    corr_col = abs(df[[col, target]].corr().iloc[0, 1])\n",
    "                    drop = row if corr_row < corr_col else col\n",
    "\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid strategy. Choose 'nulls' or 'target_corr'.\")\n",
    "\n",
    "                to_drop.add(drop)\n",
    "\n",
    "    reduced_df = df.drop(columns=list(to_drop))\n",
    "    return reduced_df, list(to_drop)\n",
    "\n",
    "# Example usage:\n",
    "df, dropped = remove_highly_correlated_features(df, target='RainTomorrow', threshold=0.9, strategy='target_corr')\n",
    "print(\"Dropped features:\", dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4903be47",
   "metadata": {},
   "source": [
    "#### 5. Handling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b295ae73",
   "metadata": {},
   "source": [
    "Impute categorical features with the label `'Missing'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d642c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[cat_features].isna().mean())\n",
    "\n",
    "for col in cat_features:\n",
    "    df[col] = df[col].fillna('Missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f12bca",
   "metadata": {},
   "source": [
    "Impute some numerical features by the median, grouped by `'Month'` and `'Location'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bfa481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_median_by_group(df, features, group_cols=['Month', 'Location']):\n",
    "    \"\"\"\n",
    "    Impute missing values in specified columns using the median,\n",
    "    calculated by grouping over the given columns (default 'Month' and 'Location').\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Original dataframe.\n",
    "        features (list): List of columns to impute.\n",
    "        group_cols (list): Columns to group by for median calculation (default ['Month', 'Location']).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with imputed values.\n",
    "    \"\"\"\n",
    "    df_imputed = df.copy()\n",
    "    \n",
    "    # Calculate group-wise medians for the features\n",
    "    medians = df_imputed.groupby(group_cols)[features].transform('median')\n",
    "    \n",
    "    # Fill missing values with the calculated medians\n",
    "    for feature in features:\n",
    "        df_imputed[feature] = df_imputed[feature].fillna(medians[feature])\n",
    "    \n",
    "    return df_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "035e8c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = df.select_dtypes(include=['number']).columns.tolist()\n",
    "#print(f\"Missing values %:\\n {df[num_features].isna().mean().sort_values(ascending=False)}\")\n",
    "#print(f\"\\n\\nCorrelation with target: \\n {df[num_features].corr()[target].sort_values()}\")\n",
    "impute_by_model = ['Sunshine', 'Cloud3pm', 'Cloud9am'] # Those are 3/4 of the features with most missing data. We exclude the feature Evaporation because that feature doesn't have too much correlation with the target (the other 3 does) \n",
    "impute_median_features = [x for x in num_features if x not in impute_by_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a545c0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = impute_median_by_group(df, impute_median_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e438a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MinTemp',\n",
       " 'Rainfall',\n",
       " 'Evaporation',\n",
       " 'WindGustSpeed',\n",
       " 'WindSpeed9am',\n",
       " 'WindSpeed3pm',\n",
       " 'Humidity9am',\n",
       " 'Humidity3pm',\n",
       " 'Pressure9am',\n",
       " 'Temp3pm',\n",
       " 'RainTomorrow',\n",
       " 'Month']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "impute_median_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "16bf258c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinTemp          0.000000\n",
       "Rainfall         0.000000\n",
       "Evaporation      0.321161\n",
       "Sunshine         0.477475\n",
       "WindGustSpeed    0.041829\n",
       "WindSpeed9am     0.000000\n",
       "WindSpeed3pm     0.000000\n",
       "Humidity9am      0.000000\n",
       "Humidity3pm      0.000000\n",
       "Pressure9am      0.083709\n",
       "Cloud9am         0.380258\n",
       "Cloud3pm         0.404455\n",
       "Temp3pm          0.000000\n",
       "RainTomorrow     0.000000\n",
       "Month            0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[num_features].isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1f50abcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Location\n",
       "Albury           2981\n",
       "Tuggeranong      2968\n",
       "SalmonGums       2935\n",
       "Penrith          2931\n",
       "Witchcliffe      2909\n",
       "Ballarat         2893\n",
       "BadgerysCreek    2890\n",
       "Newcastle        2831\n",
       "MountGinini      2792\n",
       "Walpole          2787\n",
       "GoldCoast        2749\n",
       "NorahHead        2741\n",
       "PearceRAAF       2723\n",
       "Wollongong       2717\n",
       "Nhil             1518\n",
       "Uluru            1507\n",
       "Launceston       1207\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Evaporation'].isna()]['Location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b471036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Location']=='Uluru']['Evaporation'].isna().mean()\n",
    "# Hay lugares donde no se mide la evaporacion!!!!!!!!!!!!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f33e81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a4ea86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f3e30d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e79d6c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aus_rain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
