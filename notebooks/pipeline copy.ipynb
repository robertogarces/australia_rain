{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cd1db0a",
   "metadata": {},
   "source": [
    "# Steps\n",
    "\n",
    "* Map 'RainTomorrow' and 'RainToday'\n",
    "* Remove outliers\n",
    "* Handle missing data\n",
    "* Drop unnecessary features\n",
    "* Standarize\n",
    "* Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39831a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89f7e28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data_and_save(data_path, test_size):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into training and testing sets.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to split.\n",
    "        test_size (float): Proportion of the dataset to include in the test split (between 0 and 1).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (train_df, test_df) Training and testing DataFrames.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f\"{data_path}/weatherAUS.csv\")\n",
    "    train_df, test_df = train_test_split(df, test_size=test_size, random_state=42)\n",
    "    train_df.to_csv(f\"{data_path}/train_weatherAUS.csv\", index=False)\n",
    "    test_df.to_csv(f\"{data_path}/test_weatherAUS.csv\", index=False)\n",
    "\n",
    "\n",
    "def load_features(features_path: str):\n",
    "    \"\"\"Cargar archivo YAML de configuraciÃ³n\"\"\"\n",
    "    with open(features_path, 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "def get_data(data_path: str) -> pd.DataFrame:\n",
    "    return(pd.read_csv(data_path))\n",
    "\n",
    "def load_config(config_path: str):\n",
    "    with open(config_path, 'r') as file:\n",
    "        return yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb276908",
   "metadata": {},
   "source": [
    "# now with classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49cba8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Missing value handler\n",
    "class MissingValueHandler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=0.2):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.columns_to_drop_ = X.isnull().mean()[lambda x: x > self.threshold].index.tolist()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.drop(columns=self.columns_to_drop_)\n",
    "        return X.dropna()\n",
    "\n",
    "# 2. Binary mapper\n",
    "class BinaryMapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns=None, mapping={'No': 0, 'Yes': 1}):\n",
    "        self.columns = columns\n",
    "        self.mapping = mapping\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col in self.columns or []:\n",
    "            if col in X.columns:\n",
    "                X[col] = X[col].map(self.mapping)\n",
    "        return X\n",
    "\n",
    "# 3. Outlier remover\n",
    "class OutlierRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features=None, quantile=0.99):\n",
    "        self.features = features\n",
    "        self.quantile = quantile\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.thresholds_ = {\n",
    "            feature: X[feature].quantile(self.quantile)\n",
    "            for feature in self.features or [] if feature in X.columns\n",
    "        }\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for feature, threshold in self.thresholds_.items():\n",
    "            X = X[(X[feature] < threshold) | X[feature].isnull()]\n",
    "        return X\n",
    "\n",
    "# 4. Numerical scaler\n",
    "class NumericalScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, exclude=None):\n",
    "        self.exclude = exclude\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.numerical_features_ = X.select_dtypes(include='number').columns.tolist()\n",
    "        if self.exclude in self.numerical_features_:\n",
    "            self.numerical_features_.remove(self.exclude)\n",
    "        self.scaler.fit(X[self.numerical_features_])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[self.numerical_features_] = self.scaler.transform(X[self.numerical_features_])\n",
    "        return X\n",
    "\n",
    "# 5. Categorical encoder\n",
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        self.categorical_features_ = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        self.encoders_ = {\n",
    "            col: LabelEncoder().fit(X[col].astype(str))\n",
    "            for col in self.categorical_features_\n",
    "        }\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col, encoder in self.encoders_.items():\n",
    "            X[col] = encoder.transform(X[col].astype(str))\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "594bb009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(config):\n",
    "    \"\"\"\n",
    "    Executes the data preprocessing pipeline and returns X, y, and the fitted pipeline.\n",
    "\n",
    "    Parameters:\n",
    "        config (dict): Configuration dictionary with paths and preprocessing parameters.\n",
    "\n",
    "    Returns:\n",
    "        X (pd.DataFrame): Processed features.\n",
    "        y (pd.Series): Target variable.\n",
    "        preprocessing_pipeline (Pipeline): Fitted preprocessing pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    split_data_and_save(config['raw_data_path'], test_size=0.2)\n",
    "\n",
    "    # Load data\n",
    "    df = get_data(f\"{config['raw_data_path']}/train_weatherAUS.csv\")\n",
    "\n",
    "    # Drop unnecessary features\n",
    "    df = df.drop(columns=config.get('features_to_drop', []))\n",
    "\n",
    "    # Define the preprocessing pipeline\n",
    "    preprocessing_pipeline = Pipeline(steps=[\n",
    "        ('missing', MissingValueHandler(threshold=config.get('missing_data_threshold', 0.2))),\n",
    "        ('binary', BinaryMapper(columns=config.get('features_to_map', []))),\n",
    "        ('outliers', OutlierRemover(features=config.get('features_with_outliers', []))),\n",
    "        ('scaling', NumericalScaler(exclude=config['target'])),\n",
    "        ('encoding', CategoricalEncoder()),\n",
    "    ])\n",
    "\n",
    "    # Fit and transform the pipeline\n",
    "    df_processed = preprocessing_pipeline.fit_transform(df)\n",
    "\n",
    "    joblib.dump(preprocessing_pipeline, f\"{config['artifacts_path']}/preprocessing_pipeline.joblib\")\n",
    "\n",
    "    # Separate features and target\n",
    "    X = df_processed.drop(columns=[config['target']])\n",
    "    y = df_processed[config['target']]\n",
    "\n",
    "    X.to_csv(f\"{config['processed_data_path']}/X_train.csv\", index=False)\n",
    "    y.to_csv(f\"{config['processed_data_path']}/y_train.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68fcac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config('../config/config.yaml')\n",
    "preprocessing(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5897a987",
   "metadata": {},
   "source": [
    "# Training Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c062c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_data(f\"{config['processed_data_path']}/X_train.csv\")\n",
    "y = get_data(f\"{config['processed_data_path']}/y_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9cebe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_save_model(X, y, model_path, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Splits the data, trains an XGBoost classifier, evaluates it, and saves the model.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame or np.ndarray): Feature matrix.\n",
    "        y (pd.Series or np.ndarray): Target vector.\n",
    "        model_path (str): Path to save the trained model (e.g., 'models/xgb_model.joblib').\n",
    "        test_size (float): Proportion of the dataset to include in the test split.\n",
    "\n",
    "    Returns:\n",
    "        model: The trained XGBoost model.\n",
    "    \"\"\"\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Initialize and train the model\n",
    "    model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Classification Report:\\n\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Ensure the directory exists and save the model\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    joblib.dump(model, model_path)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7901653d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertogarces/miniforge3/envs/datascience/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [17:43:45] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91     13973\n",
      "           1       0.74      0.56      0.64      4085\n",
      "\n",
      "    accuracy                           0.86     18058\n",
      "   macro avg       0.81      0.75      0.77     18058\n",
      "weighted avg       0.85      0.86      0.85     18058\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = train_evaluate_save_model(X, y, model_path=f'{config[\"models_path\"]}/model.joblib', test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2da684",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f282bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = get_data(f\"{config['raw_data_path']}/test_weatherAUS.csv\")\n",
    "test_df.drop(columns=config.get('features_to_drop', []), inplace=True)\n",
    "\n",
    "pipeline = joblib.load(f\"{config['artifacts_path']}/preprocessing_pipeline.joblib\")\n",
    "test_df = pipeline.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f38e0c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = get_data(f\"{config['processed_data_path']}/X_train.csv\")\n",
    "#y = get_data(f\"{config['processed_data_path']}/y_train.csv\")\n",
    "X = test_df.drop(config['target'], axis=1)\n",
    "y = test_df[config['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e412d0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91     17613\n",
      "           1       0.75      0.56      0.64      5026\n",
      "\n",
      "    accuracy                           0.86     22639\n",
      "   macro avg       0.81      0.75      0.78     22639\n",
      "weighted avg       0.85      0.86      0.85     22639\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_preds = model.predict(X)\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f4e75c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a4216c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d54349c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
